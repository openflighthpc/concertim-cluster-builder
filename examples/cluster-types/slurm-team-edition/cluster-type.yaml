# Brief description of the purpose of the cluster.
title: "SLURM: Team Edition"

# Fuller description of the purpose of the cluster and how to use it.  You can
# use markdown here.
description: |
  A small collaborative environment, great for teams running short projects.

instructions:
  - id: launch
    title: Launch instructions
    text: |
      It will take about 30 minutes for the cluster to launch and fully configure
      itself to be ready for usage. Progress of the application can be verified as
      complete when `flight profile list` on `gateway1` shows all nodes with a
      status of `complete`.

      Once launched, you can access the cluster via ssh to the public IP of
      `gateway1` as the user `flight` with the private key that corresponds with
      the key-pair you selected above.

  - id: admin
    title: Admin instructions
    text: |
      ### Storage Management

      The cluster configures NFS for shared storage across the login and compute
      nodes. The directories shared are as follows:

      - `/home`: For user home directories
      - `/opt/apps`: For installing shared applications
      - `/opt/data`: For user project data
      - `/opt/service`: For admin data and shared system configuration scripts
      - `/opt/site`: For any other shared data or information on the cluster or site

      Note that, while the `/home` directory is shared across the cluster, it does
      not utilise the shared storage disk of `gateway1` and therefore should not be
      used for project data

      ### Adding Users

      A test user is automatically created to verify the installation of IPA is
      successful. This user can be logged into from the `infra01` machine as the
      root user with the key generated for the `testuser` (`ssh -i
      /root/.ssh/id_testuser testuser@gateway1`).

      To add a new user:

      1. Login to `infra01` as the root user
          1. This can be done by logging into `gateway1` as the user `flight` then
          switching to the root user (`sudo su -`) and then logging into `infra01`
      1. Authorise as the IPA admin user (using the generated admin password
      available in `/root/ipa_admin_pass.txt` on `gateway1`)
          ```bash
          kinit admin
          ```
      1. Add the user along with a trusted SSH public key (this should be one that
      corresponds with a private key that the user has outside of the system)
          ```bash
          ipa user-add --cn="New User" --first=New --last=User newuser
          ```
      1. Set a temporary password for the user (this will later be changed by the
      user once they've accessed the system, **this temporary password will need to
      be shared with the new user**)
          ```bash
          ipa passwd newuser
          ```
      3. Add the user to the cluster users group to enable access
          ```bash
          ipa group-add-member cluster-users --users newuser
          ```

      Once the user has successfully logged in, they will need to set a password
      for themselves to be able to access the [Flight Web
      Suite](../docs/flight-environment/use-flight/flight-web-suite/index.md). This
      can be done by them running `kinit newuser`,
      entering their temporary password and then following the prompts to set a new one.

      For more information on managing users, see the [FreeIPA Admin
      guide](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_identity_management/managing-user-accounts-using-the-command-line_configuring-and-managing-idm).

  - id: usage
    title: Usage instructions
    text: |
      ### Accessing System

      The IP address of the `gateway1` node should be shared with you by the system
      administrator for this cluster.  Once received, see the [HPC Environment
      Basics guide to Logging
      In](https://www.openflighthpc.org/latest/docs/hpc-environment-basics/linux-usage/cli-basics/logging-in/).

      You administrator should have shared a temporary password with you, this is
      used to authenticate you to set your own password which will allow you access
      to the cluster via CLI and the [Flight Web
      Suite](https://www.openflighthpc.org/latest/docs/flight-environment/use-flight/flight-web-suite/).

      ### Copying Data Across

      The IP address of the `gateway1` node should be shared with you by the system
      administrator for this cluster.  Once received, see the [HPC Environment
      Basics guide to Working with Data and
      Files](https://www.openflighthpc.org/latest/docs/hpc-environment-basics/linux-usage/working-with-data/).

      The cluster has shared storage configured for the following directories:

      - `/home`: For user home directories. Note that this directory does not
        utilise the shared storage disk of `gateway1` and therefore should not be
        used for project data
      - `/opt/apps`: For installing shared applications
      - `/opt/data`: For user project data
      - `/opt/service`: For admin data and shared system configuration scripts
      - `/opt/site`: For any other shared data or information on the cluster or site

        Note that, while the `/home` directory is shared across the cluster, it
        does not utilise the shared storage disk of `gateway1` and therefore should
        not be used for project data

        Additionally, data can be copied to the cluster using the [Flight File
        Manager](https://www.openflighthpc.org/latest/docs/flight-environment/use-flight/flight-web-suite/file-manager/)
        in Flight Web Suite.


# The kind or type of this cluster template.
kind: heat

# Used to order the list sent to visualiser.  Cluster types with lower values
# appear earlier in the list.  If the values is the same, the order is
# undefined.
order: 150

# The URL for the logo to use for this cluster type.  This could be an absolute
# URL or an absolute path.
#
# If an absolute URL, e.g.,
# https://www.openflighthpc.org/images/slurm-team-edition.svg, the logo will be
# loaded from that URL.
#
# If an absolute path, e.g., /images/cluster-types/slurm-team-edition.svg, the
# logo will be loaded from that path on the visualiser service.
logo_url: /images/cluster-types/slurm-team-edition.svg

# The components that compose this cluster type definition.
#
# `name`: either the name of a file in the `components` subdirectory without
# the `.yaml` extension; or the absolute path to a file including extension.
#
# `optional`: whether this component is optional.
components:
  - name: network
  - name: gateway
    user_data:
      name: gateway
      write_files:
        - cloudinit.slurm-multinode
        - shared-storage-mount
        - passwords.slurm-multinode
        - post-apply-tweaks.slurm-multinode
  - name: infra
    user_data:
      name: infra
      write_files:
        - cloudinit.infra-multinode
  - name: nodes
    user_data:
      name: node
      write_files:
        - cloudinit.compute-multinode


# parameter_groups defines the parameter groups for this cluster type.
#
# It is similar to the `parameter_groups` attribute used in HOT with a single
# additional attribute `optional`.
#
# The `optional` attribute specifies that a check box or similar should be
# rendered for that parameter group. The parameters contained in the parameter
# group should only be displayed to the user if that check box is selected.
# Cluster builder will use the value of that checkbox to determine which
# optional components to include.
#
# `optional.label`   - the label to display for an optional parameter group.
# `optional.default` - whether the group is selected or not by default.
# `optional.name`    - the name of the component that will be included only if this group is selected.
parameter_groups:
  - label: "Cluster parameters"
    description: "Configuration for parameters common across the cluster"
    parameters:
      - clustername
      - solo-image
      - key_name

  - label: "Network settings"
    description: "Configuration for the external and internal cluster networks"
    parameters:
      - external-network
      - network-cidr-pri

  - label: "Gateway parameters"
    description: "Parameters for configuring the gateway"
    parameters:
      - gateway-pri-ip
      - gateway-flavour
      - storage-size

  - label: "Infrastructure node parameters"
    description: "Parameters for configuring infra01 if requested"
    parameters:
      - infra-flavour

  - label: "Compute node parameters"
    description: "Parameters for configuring compute nodes if requested"
    parameters:
      - node-flavour
      - node-count


# Any of the parameters defined in `parameters.yaml` can be hardcoded here. If
# they are hardcoded, the value provided here will be passed to Heat when
# launching the cluster.  It will not be possible for the user to override
# these values.
hardcoded_parameters:
  external-network: public1
  network-cidr-pri: '10.100.0.0/16'
  gateway-pri-ip: '10.100.0.101'
  solo-image: "Flight Solo 2024.1"
  # node-count: 5
  cluster-type: slurm-team-edition


# parameter_overrides allows for overriding any of the parameters loaded from
# any component for this particular cluster type.
#
# This might be useful for defining, say, a generic node-count parameter in
# the component template library.
#
#     node-count:
#       type: number
#       label: Number of Nodes
#       constraints:
#         - range: { min: 1 }
#
# Three cluster types could be created from this, say, small, medium and large.
# Each cluster type could override the default and constraints suitable
# for that cluster type.  E.g., the small might specify the following:
#
#     parameter_overrides:
#       node-count:
#         default: 5
#         constraints:
#           - range: { min: 1, max: 5 }
#
# This would allow the user to launch a multi-node cluster, limited to 5 nodes,
# whilst still allowing them to reduce the cost consumption if they wanted to.
#
# All values specified here replace the values specified in the template
# library, so when overriding the constraints you need to specify every desired
# constraint for that parameter.
#
# If a parameter also appears in hardcoded_parameters, that hardcoded value
# will be used without the user having the option to adjust it.  The hardcoded
# value will need to be consistent with any constraints set here or in
# the template library.
parameter_overrides:
  node-count:
    default: 5
    constraints:
      - range: { min: 1, max: 5 }
  storage-size:
    default: 1024
    constraints:
      - range: { min: 16, max: 4096 }
